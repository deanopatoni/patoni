{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deanopatoni/patoni/blob/main/Excel_to_CSV_Multi_Sheet_20250813_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__w5c3XnAA_T"
      },
      "source": [
        "# Excel to CSV Multi-TAB Converter\n",
        "`Refactoring in Python_20250813_2 Best yet!`\n",
        "\n",
        "This Python script provides a powerful yet simple way to convert Excel workbooks (XLSX files) into a single, well-organized CSV file.\n",
        "\n",
        "## What It Does\n",
        "\n",
        "* **Input:** Any Excel workbook (.xlsx file)\n",
        "* **Output:** Single organized CSV file containing all sheets\n",
        "* **Process:** Automatically extracts and combines all sheets while preserving their names and structure\n",
        "\n",
        "## Key Features\n",
        "\n",
        "* Preserves original sheet names as section headers in the CSV\n",
        "* Shows preview of each sheet's data during processing\n",
        "* Reports detailed statistics (rows, columns, memory usage)\n",
        "* Handles large Excel files efficiently\n",
        "* Provides error handling and progress updates\n",
        "* Gives user control over final download\n",
        "\n",
        "## How to Use\n",
        "\n",
        "1. Run the script\n",
        "2. Click 'Upload' when prompted\n",
        "3. Select your Excel file\n",
        "4. Review the data previews that appear\n",
        "5. Type 'yes' when asked to download\n",
        "6. Get your combined CSV file!\n",
        "\n",
        "## Output Format\n",
        "\n",
        "The resulting CSV will look like this:\n",
        "\n",
        "```\n",
        "--- Data from tab 'Sheet1' ---\n",
        "column1,column2,column3\n",
        "data,data,data\n",
        "data,data,data\n",
        "\n",
        "--- Data from tab 'Sheet2' ---\n",
        "column1,column2,column3\n",
        "data,data,data\n",
        "data,data,data\n",
        "```\n",
        "\n",
        "## Common Use Cases\n",
        "\n",
        "* Combining multiple Excel sheets into one file\n",
        "* Converting Excel data for database imports\n",
        "* Creating text-based backups of Excel workbooks\n",
        "* Sharing data with CSV-only systems\n",
        "* Analyzing multiple sheets of data together\n",
        "\n",
        "## Requirements\n",
        "\n",
        "* Python 3.x\n",
        "* Required libraries: openpyxl, pandas\n",
        "* Google Colab environment (for the upload/download functionality)\n",
        "\n",
        "---\n",
        "*Note: This script is designed to run in Google Colab and uses Colab's built-in file handling capabilities.*\n",
        "\n",
        "Major Improvements v20250813_2:\n",
        "1. Memory Management\n",
        "\n",
        "Added explicit workbook cleanup with workbook.close() in a finally block\n",
        "Used memory-efficient openpyxl loading options\n",
        "\n",
        "2. CSV Header Handling\n",
        "\n",
        "Replaced problematic section headers with clean separator rows\n",
        "Used write_dataframe_with_separator() for consistent formatting\n",
        "Avoided mixing write_to_csv() and DataFrame.to_csv()\n",
        "\n",
        "3. Data Cleaning\n",
        "\n",
        "Added clean_dataframe() function to remove empty rows/columns\n",
        "Handles whitespace-only content and null values properly\n",
        "\n",
        "4. Flexible Output Naming\n",
        "\n",
        "Added optional output_file parameter to process_excel_file()\n",
        "Optional tempfile support with use_tempfile parameter\n",
        "\n",
        "5. Consistent Logging\n",
        "\n",
        "Replaced all print() statements with logging.info()\n",
        "Added structured logging with clear sections and progress indicators\n",
        "\n",
        "6. Enhanced User Input\n",
        "\n",
        "Created get_user_confirmation() function for clean, validated input\n",
        "Single strip/lower operation with proper error handling\n",
        "Support for keyboard interruption\n",
        "\n",
        "7. Sheet Name Edge Cases\n",
        "\n",
        "Added generate_unique_sheet_names() to handle duplicate sheet names\n",
        "Appends numbers to duplicates (e.g., \"Sheet1\", \"Sheet1_2\", \"Sheet1_3\")\n",
        "\n",
        "8. Better Error Handling\n",
        "\n",
        "More granular try-catch blocks\n",
        "Detailed error reporting for each processing stage\n",
        "Graceful handling of empty sheets\n",
        "\n",
        "9. Enhanced UX\n",
        "\n",
        "Better progress reporting with sheet counters\n",
        "File size information for uploads and downloads\n",
        "Clear success/failure indicators with ✓/✗ symbols\n",
        "Professional formatting with section dividers\n",
        "\n",
        "10. Modular Architecture\n",
        "\n",
        "Clean separation of concerns with focused functions\n",
        "Type hints for better code documentation\n",
        "Comprehensive docstrings for all functions\n",
        "\n",
        "Key Features:\n",
        "\n",
        "Memory Efficient: Proper cleanup and optimized pandas settings\n",
        "Robust: Handles edge cases like empty sheets, duplicate names, and malformed data\n",
        "User-Friendly: Clear progress updates and professional output formatting\n",
        "Flexible: Configurable output naming and preview options\n",
        "Production-Ready: Comprehensive error handling and logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "WqZBOpzC_T0M",
        "outputId": "ee4f16b7-7248-4b36-ac0a-a136e8a4500b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8c37b5c1-6be4-481f-a37a-431f986d5a8b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8c37b5c1-6be4-481f-a37a-431f986d5a8b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import openpyxl\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import time\n",
        "import logging\n",
        "import tempfile\n",
        "import os\n",
        "from collections import Counter\n",
        "from typing import Optional, Tuple, List\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Set pandas display options for better output readability\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Clean DataFrame by removing fully empty rows and columns.\n",
        "\n",
        "    Args:\n",
        "        df: Input DataFrame to clean\n",
        "\n",
        "    Returns:\n",
        "        Cleaned DataFrame\n",
        "    \"\"\"\n",
        "    # Drop rows where all values are NaN or empty\n",
        "    df_cleaned = df.dropna(how='all')\n",
        "\n",
        "    # Drop columns where all values are NaN or empty\n",
        "    df_cleaned = df_cleaned.dropna(axis=1, how='all')\n",
        "\n",
        "    # Remove columns that are entirely whitespace or empty strings\n",
        "    for col in df_cleaned.columns:\n",
        "        if df_cleaned[col].dtype == 'object':\n",
        "            # Check if all non-null values are empty strings or whitespace\n",
        "            non_null_values = df_cleaned[col].dropna()\n",
        "            if len(non_null_values) > 0:\n",
        "                if all(str(val).strip() == '' for val in non_null_values):\n",
        "                    df_cleaned = df_cleaned.drop(columns=[col])\n",
        "\n",
        "    return df_cleaned\n",
        "\n",
        "def generate_unique_sheet_names(sheet_names: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate unique sheet names by appending numbers to duplicates.\n",
        "\n",
        "    Args:\n",
        "        sheet_names: List of original sheet names\n",
        "\n",
        "    Returns:\n",
        "        List of unique sheet names\n",
        "    \"\"\"\n",
        "    name_counts = Counter(sheet_names)\n",
        "    unique_names = []\n",
        "    name_usage = {}\n",
        "\n",
        "    for name in sheet_names:\n",
        "        if name_counts[name] == 1:\n",
        "            unique_names.append(name)\n",
        "        else:\n",
        "            # Handle duplicates by appending numbers\n",
        "            if name not in name_usage:\n",
        "                name_usage[name] = 0\n",
        "            name_usage[name] += 1\n",
        "            unique_name = f\"{name}_{name_usage[name]}\"\n",
        "            unique_names.append(unique_name)\n",
        "\n",
        "    return unique_names\n",
        "\n",
        "def process_excel_sheet(sheet, sheet_name: str, sheet_number: int) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Process a single Excel sheet and return its cleaned DataFrame.\n",
        "\n",
        "    Args:\n",
        "        sheet: openpyxl worksheet object\n",
        "        sheet_name: Name of the sheet\n",
        "        sheet_number: Sequential number of the sheet\n",
        "\n",
        "    Returns:\n",
        "        Processed DataFrame or None if processing failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(f\"Reading data from '{sheet_name}'...\")\n",
        "\n",
        "        # Extract all data from the sheet\n",
        "        data = [[cell for cell in row] for row in sheet.iter_rows(values_only=True)]\n",
        "        if not data:\n",
        "            logging.warning(f\"No data found in tab '{sheet_name}'\")\n",
        "            return None\n",
        "\n",
        "        # Separate headers and data rows\n",
        "        columns = data[0] if data else []\n",
        "        rows = data[1:] if len(data) > 1 else []\n",
        "\n",
        "        # Create DataFrame\n",
        "        df = pd.DataFrame(rows, columns=columns)\n",
        "\n",
        "        # Clean column names by stripping whitespace\n",
        "        df.columns = [str(col).strip() if col is not None else f\"Unnamed_{i}\"\n",
        "                      for i, col in enumerate(df.columns)]\n",
        "\n",
        "        # Clean the DataFrame by removing empty rows and columns\n",
        "        df = clean_dataframe(df)\n",
        "\n",
        "        # Skip if DataFrame is empty after cleaning\n",
        "        if df.empty:\n",
        "            logging.warning(f\"Tab '{sheet_name}' is empty after cleaning\")\n",
        "            return None\n",
        "\n",
        "        # Add metadata columns at the beginning\n",
        "        df.insert(0, 'Sheet_Number', sheet_number)\n",
        "        df.insert(1, 'Tab', sheet_name)\n",
        "        df.insert(2, 'Index', range(1, len(df) + 1))\n",
        "\n",
        "        logging.info(f\"Extracted {len(df)} rows and {len(df.columns)} columns from '{sheet_name}'\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing tab '{sheet_name}': {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def preview_dataframe(df: pd.DataFrame, sheet_name: str, num_rows: int = 5) -> None:\n",
        "    \"\"\"\n",
        "    Display a preview of the DataFrame using logging.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame to preview\n",
        "        sheet_name: Name of the sheet for display\n",
        "        num_rows: Number of rows to show in preview\n",
        "    \"\"\"\n",
        "    total_columns = len(df.columns)\n",
        "    preview_columns = min(10, total_columns)\n",
        "\n",
        "    logging.info(f\"\\nPreview of '{sheet_name}' (first {num_rows} rows):\")\n",
        "\n",
        "    # Convert preview to string for consistent logging output\n",
        "    preview_df = df.iloc[:num_rows, :preview_columns]\n",
        "    logging.info(f\"\\n{preview_df.to_string()}\")\n",
        "\n",
        "    if total_columns > 10:\n",
        "        logging.info(f\"\\nNote: {total_columns - 10} additional columns not shown in preview\")\n",
        "\n",
        "    logging.info(f\"\\nTotal rows: {len(df)}\")\n",
        "    logging.info(f\"Total columns: {total_columns}\")\n",
        "    logging.info(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
        "    logging.info(\"\\n\" + \"=\"*50)\n",
        "\n",
        "def write_dataframe_with_separator(df: pd.DataFrame, output_file: str, sheet_name: str,\n",
        "                                   is_first_sheet: bool = False) -> None:\n",
        "    \"\"\"\n",
        "    Write DataFrame to CSV with optional separator row.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame to write\n",
        "        output_file: Path to output CSV file\n",
        "        sheet_name: Name of the sheet for separator\n",
        "        is_first_sheet: Whether this is the first sheet being written\n",
        "    \"\"\"\n",
        "    mode = 'w' if is_first_sheet else 'a'\n",
        "\n",
        "    # Add separator row if not the first sheet\n",
        "    if not is_first_sheet:\n",
        "        separator_row = pd.DataFrame([[''] * len(df.columns)], columns=df.columns)\n",
        "        separator_row.iloc[0, 0] = f\"--- Data from tab '{sheet_name}' ---\"\n",
        "        separator_row.to_csv(output_file, mode='a', header=False, index=False)\n",
        "\n",
        "    # Write the actual data\n",
        "    df.to_csv(output_file, mode=mode, header=is_first_sheet, index=False)\n",
        "\n",
        "def write_processing_summary(output_file: str, processed_tabs: List[str],\n",
        "                           total_tabs: int, processing_time: float) -> bool:\n",
        "    \"\"\"\n",
        "    Write processing summary to the CSV file.\n",
        "\n",
        "    Args:\n",
        "        output_file: Path to output CSV file\n",
        "        processed_tabs: List of successfully processed tab names\n",
        "        total_tabs: Total number of tabs in workbook\n",
        "        processing_time: Time taken for processing in seconds\n",
        "\n",
        "    Returns:\n",
        "        True if successful, False otherwise\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Add separator before summary\n",
        "        with open(output_file, 'a', newline='', encoding='utf-8') as f:\n",
        "            f.write('\\n')\n",
        "            f.write('--- Processing Summary ---\\n')\n",
        "\n",
        "        # Write processing statistics\n",
        "        summary_data = {\n",
        "            'Metric': ['Total tabs in workbook', 'Successfully processed tabs', 'Processing time (seconds)'],\n",
        "            'Value': [total_tabs, len(processed_tabs), f\"{processing_time:.2f}\"]\n",
        "        }\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        summary_df.to_csv(output_file, mode='a', index=False, header=True)\n",
        "\n",
        "        # Write list of processed tabs\n",
        "        if processed_tabs:\n",
        "            with open(output_file, 'a', newline='', encoding='utf-8') as f:\n",
        "                f.write('\\n--- Processed Tabs ---\\n')\n",
        "\n",
        "            tab_summary = pd.DataFrame({\n",
        "                'Sheet_Number': range(1, len(processed_tabs) + 1),\n",
        "                'Tab_Name': processed_tabs\n",
        "            })\n",
        "            tab_summary.to_csv(output_file, mode='a', index=False, header=True)\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error writing processing summary: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def process_excel_file(file_path: str, output_file: Optional[str] = None,\n",
        "                      preview: bool = True, use_tempfile: bool = False) -> Tuple[bool, Optional[str]]:\n",
        "    \"\"\"\n",
        "    Process Excel file and save all sheets to a single CSV.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the Excel file\n",
        "        output_file: Optional custom output filename\n",
        "        preview: Whether to show data previews\n",
        "        use_tempfile: Whether to use temporary file for output\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (success_flag, output_file_path)\n",
        "    \"\"\"\n",
        "    logging.info(f\"Opening workbook: {file_path}\")\n",
        "    start_time = time.time()\n",
        "    workbook = None\n",
        "\n",
        "    try:\n",
        "        # Load workbook with memory-efficient settings\n",
        "        workbook = openpyxl.load_workbook(file_path, read_only=True, data_only=True)\n",
        "\n",
        "        # Generate output filename\n",
        "        if output_file is None:\n",
        "            timestamp = int(time.time())\n",
        "            if use_tempfile:\n",
        "                temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False)\n",
        "                output_file = temp_file.name\n",
        "                temp_file.close()\n",
        "            else:\n",
        "                output_file = f\"all_tabs_data_{timestamp}.csv\"\n",
        "\n",
        "        # Get sheet names and make them unique\n",
        "        original_sheet_names = workbook.sheetnames\n",
        "        unique_sheet_names = generate_unique_sheet_names(original_sheet_names)\n",
        "\n",
        "        processed_tabs = []\n",
        "        failed_tabs = []\n",
        "        total_tabs = len(original_sheet_names)\n",
        "\n",
        "        logging.info(f\"Found {total_tabs} sheets to process\")\n",
        "\n",
        "        # Process each sheet\n",
        "        for idx, (original_name, unique_name) in enumerate(zip(original_sheet_names, unique_sheet_names)):\n",
        "            logging.info(f\"\\n--- Processing tab '{original_name}' ({idx+1}/{total_tabs}) ---\")\n",
        "\n",
        "            if original_name != unique_name:\n",
        "                logging.info(f\"Renamed duplicate sheet to '{unique_name}'\")\n",
        "\n",
        "            try:\n",
        "                sheet = workbook[original_name]\n",
        "                df = process_excel_sheet(sheet, unique_name, idx + 1)\n",
        "\n",
        "                if df is not None and not df.empty:\n",
        "                    if preview:\n",
        "                        preview_dataframe(df, unique_name)\n",
        "\n",
        "                    # Write data to CSV with separator\n",
        "                    write_dataframe_with_separator(df, output_file, unique_name,\n",
        "                                                 is_first_sheet=(idx == 0))\n",
        "                    processed_tabs.append(unique_name)\n",
        "                    logging.info(f\"Successfully saved data for sheet '{unique_name}'\")\n",
        "                else:\n",
        "                    logging.warning(f\"Skipping empty sheet '{unique_name}'\")\n",
        "                    failed_tabs.append(unique_name)\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error processing sheet '{original_name}': {str(e)}\")\n",
        "                failed_tabs.append(unique_name)\n",
        "\n",
        "        # Calculate processing time\n",
        "        end_time = time.time()\n",
        "        processing_time = end_time - start_time\n",
        "\n",
        "        # Log final statistics\n",
        "        logging.info(f\"\\n{'='*60}\")\n",
        "        logging.info(f\"PROCESSING COMPLETE\")\n",
        "        logging.info(f\"{'='*60}\")\n",
        "        logging.info(f\"Total processing time: {processing_time:.2f} seconds\")\n",
        "        logging.info(f\"Successfully processed: {len(processed_tabs)}/{total_tabs} sheets\")\n",
        "\n",
        "        if processed_tabs:\n",
        "            logging.info(f\"\\nSuccessfully processed tabs:\")\n",
        "            for tab in processed_tabs:\n",
        "                logging.info(f\"  ✓ {tab}\")\n",
        "\n",
        "        if failed_tabs:\n",
        "            logging.warning(f\"\\nFailed to process tabs:\")\n",
        "            for tab in failed_tabs:\n",
        "                logging.warning(f\"  ✗ {tab}\")\n",
        "\n",
        "        # Write processing summary to file\n",
        "        if processed_tabs:  # Only write summary if we processed at least one sheet\n",
        "            summary_success = write_processing_summary(output_file, processed_tabs,\n",
        "                                                     total_tabs, processing_time)\n",
        "            if summary_success:\n",
        "                logging.info(f\"\\nProcessing summary written to: {output_file}\")\n",
        "\n",
        "            return True, output_file\n",
        "        else:\n",
        "            logging.error(\"No sheets were successfully processed\")\n",
        "            return False, None\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing workbook: {str(e)}\")\n",
        "        return False, None\n",
        "    finally:\n",
        "        # Always close the workbook to free memory\n",
        "        if workbook is not None:\n",
        "            try:\n",
        "                workbook.close()\n",
        "                logging.info(\"Workbook closed and memory freed\")\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Error closing workbook: {str(e)}\")\n",
        "\n",
        "def get_user_confirmation(prompt: str, valid_responses: List[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Get user input with validation and consistent formatting.\n",
        "\n",
        "    Args:\n",
        "        prompt: Prompt message to display\n",
        "        valid_responses: List of valid responses (default: ['yes', 'no'])\n",
        "\n",
        "    Returns:\n",
        "        Validated user response\n",
        "    \"\"\"\n",
        "    if valid_responses is None:\n",
        "        valid_responses = ['yes', 'no']\n",
        "\n",
        "    valid_responses_lower = [resp.lower() for resp in valid_responses]\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            response = input(f\"\\n{prompt} \").strip().lower()\n",
        "            if response in valid_responses_lower:\n",
        "                return response\n",
        "            else:\n",
        "                valid_options = \"', '\".join(valid_responses)\n",
        "                logging.warning(f\"Please enter one of: '{valid_options}'\")\n",
        "        except (EOFError, KeyboardInterrupt):\n",
        "            logging.info(\"\\nOperation cancelled by user\")\n",
        "            return 'no'\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to orchestrate the Excel to CSV conversion process.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"=\"*60)\n",
        "        logging.info(\"EXCEL TO CSV MULTI-SHEET CONVERTER\")\n",
        "        logging.info(\"=\"*60)\n",
        "        logging.info(\"Please upload your Excel file:\")\n",
        "\n",
        "        # Handle file upload\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        if not uploaded:\n",
        "            logging.error(\"No file uploaded. Exiting.\")\n",
        "            return\n",
        "\n",
        "        file_path = list(uploaded.keys())[0]\n",
        "        file_size = len(uploaded[file_path])\n",
        "        logging.info(f\"Uploaded file: {file_path} ({file_size:,} bytes)\")\n",
        "\n",
        "        # Process the Excel file\n",
        "        success, output_file = process_excel_file(file_path, preview=True, use_tempfile=False)\n",
        "\n",
        "        if success and output_file:\n",
        "            # Get file size for download info\n",
        "            output_size = os.path.getsize(output_file)\n",
        "            logging.info(f\"\\nCSV file created: {output_file} ({output_size:,} bytes)\")\n",
        "\n",
        "            # Ask user if they want to download\n",
        "            download_choice = get_user_confirmation(\n",
        "                \"Do you want to download the CSV file? (yes/no):\",\n",
        "                ['yes', 'no']\n",
        "            )\n",
        "\n",
        "            if download_choice == 'yes':\n",
        "                try:\n",
        "                    files.download(output_file)\n",
        "                    logging.info(f\"File '{output_file}' has been prepared for download.\")\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Error downloading file: {str(e)}\")\n",
        "            else:\n",
        "                logging.info(f\"Download skipped. File saved as: {output_file}\")\n",
        "        else:\n",
        "            logging.error(\"Failed to process Excel file. Please check the file format and try again.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An unexpected error occurred: {str(e)}\")\n",
        "        logging.info(\"Please ensure you have uploaded a valid Excel file (.xlsx format)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y-kcwJsqiggs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqfhzzUM__R3"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}